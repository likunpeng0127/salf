# ğŸ¯ SAM (Segment Anything Model) å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸ“¦ GitHub ä»“åº“

**å®˜æ–¹åœ°å€**: https://github.com/facebookresearch/segment-anything

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/facebookresearch/segment-anything.git
cd segment-anything
```

---

## ğŸ”§ å®‰è£…æ­¥éª¤

### æ–¹æ³•1ï¼šæœ€ç®€å•å®‰è£… (æ¨è)

```bash
# 1. å®‰è£… SAM
pip install segment-anything

# 2. å®‰è£…ä¾èµ–
pip install opencv-python matplotlib

# 3. ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ (é€‰ä¸€ä¸ª)
# ViT-H (æœ€å¤§æœ€å‡†ï¼Œ2.4GB)
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth

# ViT-L (ä¸­ç­‰ï¼Œ1.2GB)
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth

# ViT-B (æœ€å°æœ€å¿«ï¼Œ375MB) â­ æ¨èå…ˆç”¨è¿™ä¸ª
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth
```

### æ–¹æ³•2ï¼šä»æºç å®‰è£…

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/facebookresearch/segment-anything.git
cd segment-anything

# 2. å®‰è£…
pip install -e .

# 3. å®‰è£…é¢å¤–ä¾èµ–
pip install opencv-python matplotlib
```

---

## ğŸ® åŸºç¡€ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1: ç‚¹å‡»åˆ†å‰² (æœ€å¸¸ç”¨)

```python
import numpy as np
import torch
import matplotlib.pyplot as plt
import cv2
from segment_anything import sam_model_registry, SamPredictor

# 1. åŠ è½½æ¨¡å‹
sam_checkpoint = "sam_vit_b_01ec64.pth"  # æ¨¡å‹æ–‡ä»¶è·¯å¾„
model_type = "vit_b"  # å¯¹åº”æ¨¡å‹ç±»å‹

device = "cuda" if torch.cuda.is_available() else "cpu"  # Macç”¨ "mps" æˆ– "cpu"
sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
sam.to(device=device)

predictor = SamPredictor(sam)

# 2. åŠ è½½å›¾ç‰‡
image = cv2.imread('your_image.jpg')
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# 3. è®¾ç½®å›¾ç‰‡
predictor.set_image(image)

# 4. ç‚¹å‡»åæ ‡è¿›è¡Œåˆ†å‰² (ä¾‹å¦‚ç‚¹å‡»è¡£æœ)
input_point = np.array([[500, 375]])  # x=500, y=375
input_label = np.array([1])  # 1 è¡¨ç¤ºå‰æ™¯ç‚¹

# 5. é¢„æµ‹
masks, scores, logits = predictor.predict(
    point_coords=input_point,
    point_labels=input_label,
    multimask_output=True,  # ç”Ÿæˆå¤šä¸ªå€™é€‰mask
)

# 6. æ˜¾ç¤ºç»“æœ
plt.figure(figsize=(10, 10))
plt.imshow(image)
plt.imshow(masks[0], alpha=0.5)  # åŠé€æ˜æ˜¾ç¤ºmask
plt.axis('off')
plt.show()

print(f"ç”Ÿæˆäº† {len(masks)} ä¸ªmaskï¼Œç½®ä¿¡åº¦: {scores}")
```

### ç¤ºä¾‹2: è‡ªåŠ¨åˆ†å‰²æ•´å¼ å›¾ç‰‡

```python
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator

# 1. åŠ è½½æ¨¡å‹
sam = sam_model_registry["vit_b"](checkpoint="sam_vit_b_01ec64.pth")
sam.to(device="cpu")

# 2. åˆ›å»ºè‡ªåŠ¨maskç”Ÿæˆå™¨
mask_generator = SamAutomaticMaskGenerator(sam)

# 3. åŠ è½½å›¾ç‰‡
image = cv2.imread('your_image.jpg')
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# 4. è‡ªåŠ¨ç”Ÿæˆæ‰€æœ‰masks
masks = mask_generator.generate(image)

print(f"æ£€æµ‹åˆ° {len(masks)} ä¸ªç‰©ä½“")

# 5. å¯è§†åŒ–
def show_anns(anns):
    if len(anns) == 0:
        return
    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)
    
    img = np.ones((sorted_anns[0]['segmentation'].shape[0], 
                   sorted_anns[0]['segmentation'].shape[1], 4))
    img[:,:,3] = 0
    
    for ann in sorted_anns:
        m = ann['segmentation']
        color_mask = np.concatenate([np.random.random(3), [0.35]])
        img[m] = color_mask
    
    plt.imshow(img)

plt.figure(figsize=(20,20))
plt.imshow(image)
show_anns(masks)
plt.axis('off')
plt.show()
```

---

## ğŸ¨ æœè£…é«˜äº®äº¤äº’ç¤ºä¾‹ (æ‚¨çš„éœ€æ±‚)

```python
import cv2
import numpy as np
from segment_anything import sam_model_registry, SamPredictor

class ClothingHighlighter:
    def __init__(self, model_path="sam_vit_b_01ec64.pth"):
        """åˆå§‹åŒ–SAMæ¨¡å‹"""
        sam = sam_model_registry["vit_b"](checkpoint=model_path)
        sam.to(device="cpu")
        self.predictor = SamPredictor(sam)
        self.image = None
        self.masks = {}
        
    def load_image(self, image_path):
        """åŠ è½½å›¾ç‰‡"""
        self.image = cv2.imread(image_path)
        self.image = cv2.cvtColor(self.image, cv2.COLOR_BGR2RGB)
        self.predictor.set_image(self.image)
        
    def segment_at_point(self, x, y):
        """ç‚¹å‡»æŸä¸ªç‚¹è¿›è¡Œåˆ†å‰²"""
        input_point = np.array([[x, y]])
        input_label = np.array([1])
        
        masks, scores, _ = self.predictor.predict(
            point_coords=input_point,
            point_labels=input_label,
            multimask_output=False
        )
        
        return masks[0]  # è¿”å›æœ€ä½³mask
    
    def highlight_region(self, mask, color=(255, 255, 0), alpha=0.5):
        """é«˜äº®æ˜¾ç¤ºåŒºåŸŸ"""
        highlighted = self.image.copy()
        colored_mask = np.zeros_like(highlighted)
        colored_mask[mask] = color
        
        result = cv2.addWeighted(highlighted, 1, colored_mask, alpha, 0)
        return result

# ä½¿ç”¨ç¤ºä¾‹
highlighter = ClothingHighlighter("sam_vit_b_01ec64.pth")
highlighter.load_image("person.jpg")

# ç”¨æˆ·ç‚¹å‡»äº†è¡£æœä½ç½® (x=300, y=200)
mask = highlighter.segment_at_point(300, 200)

# é«˜äº®æ˜¾ç¤º
result = highlighter.highlight_region(mask, color=(255, 255, 0), alpha=0.6)

# æ˜¾ç¤º
import matplotlib.pyplot as plt
plt.figure(figsize=(15, 5))
plt.subplot(1, 2, 1)
plt.imshow(highlighter.image)
plt.title('åŸå›¾')
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(result)
plt.title('é«˜äº®æ•ˆæœ')
plt.axis('off')
plt.show()
```

---

## ğŸŒ Web äº¤äº’ç‰ˆæœ¬ (æµè§ˆå™¨ä¸­è¿è¡Œ)

SAM ä¹Ÿæ”¯æŒåœ¨æµè§ˆå™¨ä¸­è¿è¡Œï¼

### JavaScript ç‰ˆæœ¬

```bash
# å…‹éš† SAM Web Demo
git clone https://github.com/facebookresearch/segment-anything.git
cd segment-anything/demo

# å®‰è£…ä¾èµ–
npm install

# å¯åŠ¨æœåŠ¡
npm run dev

# è®¿é—® http://localhost:3000
```

### ä½¿ç”¨ ONNX ç‰ˆæœ¬ (æ›´å¿«)

```python
# å¯¼å‡ºä¸º ONNX æ ¼å¼
from segment_anything import sam_model_registry, SamPredictor
from segment_anything.utils.onnx import SamOnnxModel
import torch

# 1. åŠ è½½æ¨¡å‹
sam = sam_model_registry["vit_b"](checkpoint="sam_vit_b_01ec64.pth")

# 2. å¯¼å‡ºä¸º ONNX
onnx_model = SamOnnxModel(sam, return_single_mask=True)

dynamic_axes = {
    "point_coords": {1: "num_points"},
    "point_labels": {1: "num_points"},
}

torch.onnx.export(
    onnx_model,
    (
        torch.randn(1, 3, 1024, 1024),
        torch.randn(1, 2, 2),
        torch.randn(1, 2)
    ),
    "sam_onnx_model.onnx",
    export_params=True,
    opset_version=17,
    input_names=["images", "point_coords", "point_labels"],
    output_names=["masks", "iou_predictions"],
    dynamic_axes=dynamic_axes,
)

print("âœ… ONNX æ¨¡å‹å·²å¯¼å‡ºï¼Œå¯åœ¨æµè§ˆå™¨ä¸­ä½¿ç”¨ï¼")
```

---

## ğŸ’¡ æœè£…äº¤äº’åº”ç”¨å®Œæ•´ç¤ºä¾‹

### Python + OpenCV é¼ æ ‡äº¤äº’

```python
import cv2
import numpy as np
from segment_anything import sam_model_registry, SamPredictor

class InteractiveClothingDetector:
    def __init__(self, image_path, model_path="sam_vit_b_01ec64.pth"):
        # åŠ è½½SAM
        sam = sam_model_registry["vit_b"](checkpoint=model_path)
        sam.to(device="cpu")
        self.predictor = SamPredictor(sam)
        
        # åŠ è½½å›¾ç‰‡
        self.original = cv2.imread(image_path)
        self.original = cv2.cvtColor(self.original, cv2.COLOR_BGR2RGB)
        self.display = self.original.copy()
        self.predictor.set_image(self.original)
        
        # çŠ¶æ€
        self.current_mask = None
        self.window_name = "æœè£…äº¤äº’æ£€æµ‹ - ç‚¹å‡»é€‰æ‹©æœè£…éƒ¨ä½"
        
    def mouse_callback(self, event, x, y, flags, param):
        """é¼ æ ‡å›è°ƒå‡½æ•°"""
        if event == cv2.EVENT_LBUTTONDOWN:  # å·¦é”®ç‚¹å‡»
            print(f"ç‚¹å‡»ä½ç½®: ({x}, {y})")
            
            # åˆ†å‰²
            input_point = np.array([[x, y]])
            input_label = np.array([1])
            
            masks, scores, _ = self.predictor.predict(
                point_coords=input_point,
                point_labels=input_label,
                multimask_output=False
            )
            
            self.current_mask = masks[0]
            
            # é«˜äº®æ˜¾ç¤º
            self.display = self.original.copy()
            colored_mask = np.zeros_like(self.display)
            colored_mask[self.current_mask] = (255, 255, 0)  # é»„è‰²
            self.display = cv2.addWeighted(self.display, 1, colored_mask, 0.5, 0)
            
            # ç»˜åˆ¶ç‚¹å‡»ç‚¹
            cv2.circle(self.display, (x, y), 5, (255, 0, 0), -1)
            
            print(f"âœ… åˆ†å‰²å®Œæˆï¼Œç½®ä¿¡åº¦: {scores[0]:.2f}")
            
        elif event == cv2.EVENT_RBUTTONDOWN:  # å³é”®æ¸…é™¤
            self.display = self.original.copy()
            self.current_mask = None
            print("æ¸…é™¤é«˜äº®")
    
    def run(self):
        """è¿è¡Œäº¤äº’ç•Œé¢"""
        cv2.namedWindow(self.window_name)
        cv2.setMouseCallback(self.window_name, self.mouse_callback)
        
        print("=" * 50)
        print("äº¤äº’æŒ‡å—:")
        print("  - å·¦é”®ç‚¹å‡»: é€‰æ‹©å¹¶é«˜äº®æœè£…éƒ¨ä½")
        print("  - å³é”®ç‚¹å‡»: æ¸…é™¤é«˜äº®")
        print("  - æŒ‰ 'q' æˆ– ESC: é€€å‡º")
        print("  - æŒ‰ 's': ä¿å­˜å½“å‰ç»“æœ")
        print("=" * 50)
        
        while True:
            # æ˜¾ç¤º
            display_bgr = cv2.cvtColor(self.display, cv2.COLOR_RGB2BGR)
            cv2.imshow(self.window_name, display_bgr)
            
            # æŒ‰é”®å¤„ç†
            key = cv2.waitKey(1) & 0xFF
            
            if key == ord('q') or key == 27:  # 'q' æˆ– ESC
                break
            elif key == ord('s') and self.current_mask is not None:  # 's'
                output_path = "highlighted_result.jpg"
                cv2.imwrite(output_path, display_bgr)
                print(f"âœ… å·²ä¿å­˜åˆ°: {output_path}")
        
        cv2.destroyAllWindows()

# ä½¿ç”¨
if __name__ == "__main__":
    detector = InteractiveClothingDetector(
        image_path="your_image.jpg",
        model_path="sam_vit_b_01ec64.pth"
    )
    detector.run()
```

---

## ğŸš€ åœ¨ Colab ä¸­è¿è¡Œ

```python
# ========== Colab ä¸­è¿è¡Œ SAM ==========

# 1. å®‰è£…
!pip install -q segment-anything opencv-python matplotlib

# 2. ä¸‹è½½æ¨¡å‹ (ViT-Bï¼Œæœ€å¿«)
!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth

# 3. ä¸Šä¼ å›¾ç‰‡
from google.colab import files
uploaded = files.upload()
image_path = list(uploaded.keys())[0]

# 4. è¿è¡Œåˆ†å‰²
import numpy as np
import torch
import matplotlib.pyplot as plt
import cv2
from segment_anything import sam_model_registry, SamPredictor

# åŠ è½½æ¨¡å‹
sam = sam_model_registry["vit_b"](checkpoint="sam_vit_b_01ec64.pth")
sam.to(device="cuda" if torch.cuda.is_available() else "cpu")
predictor = SamPredictor(sam)

# åŠ è½½å›¾ç‰‡
image = cv2.imread(image_path)
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
predictor.set_image(image)

# ç‚¹å‡»åˆ†å‰² (ä¿®æ”¹è¿™ä¸ªåæ ‡ä¸ºæ‚¨æƒ³ç‚¹å‡»çš„ä½ç½®)
input_point = np.array([[500, 300]])  # x=500, y=300
input_label = np.array([1])

masks, scores, _ = predictor.predict(
    point_coords=input_point,
    point_labels=input_label,
    multimask_output=True,
)

# æ˜¾ç¤ºç»“æœ
fig, axes = plt.subplots(1, 4, figsize=(20, 5))

axes[0].imshow(image)
axes[0].set_title('åŸå›¾')
axes[0].axis('off')

for i in range(3):
    axes[i+1].imshow(image)
    axes[i+1].imshow(masks[i], alpha=0.5)
    axes[i+1].set_title(f'Mask {i+1} (score: {scores[i]:.2f})')
    axes[i+1].scatter(input_point[:, 0], input_point[:, 1], 
                      color='red', s=100, marker='*')
    axes[i+1].axis('off')

plt.tight_layout()
plt.show()

print("âœ… åˆ†å‰²å®Œæˆï¼")
```

---

## âš¡ MobileSAM (æ›´å¿«ç‰ˆæœ¬)

å¦‚æœè§‰å¾—SAMå¤ªæ…¢ï¼Œå¯ä»¥ç”¨ MobileSAMï¼ˆé€Ÿåº¦æå‡60å€ï¼ï¼‰

```bash
# GitHub: https://github.com/ChaoningZhang/MobileSAM

# å®‰è£…
pip install mobile-sam

# ä¸‹è½½æ¨¡å‹
wget https://github.com/ChaoningZhang/MobileSAM/raw/master/weights/mobile_sam.pt

# ä½¿ç”¨ï¼ˆAPIå®Œå…¨ä¸€æ ·ï¼‰
from mobile_sam import sam_model_registry, SamPredictor

sam = sam_model_registry["vit_t"](checkpoint="mobile_sam.pt")
predictor = SamPredictor(sam)

# åç»­ä»£ç å®Œå…¨ç›¸åŒï¼
```

---

## ğŸ“Š æ¨¡å‹å¯¹æ¯”

| æ¨¡å‹ | å¤§å° | é€Ÿåº¦ | ç²¾åº¦ | æ¨èåœºæ™¯ |
|------|------|------|------|---------|
| **SAM ViT-H** | 2.4GB | æ…¢ | æœ€é«˜ | å¯¹ç²¾åº¦è¦æ±‚æé«˜ |
| **SAM ViT-L** | 1.2GB | ä¸­ | é«˜ | å¹³è¡¡é€‰æ‹© |
| **SAM ViT-B** | 375MB | ä¸­ | å¥½ | â­ é€šç”¨æ¨è |
| **MobileSAM** | 40MB | å¿«âš¡ | å¥½ | â­ å®æ—¶åº”ç”¨ |

---

## ğŸ¯ å¸¸è§é—®é¢˜

### Q1: Mac M1/M2 èƒ½ç”¨å—ï¼Ÿ
```python
# å¯ä»¥ï¼ä½¿ç”¨ MPS åŠ é€Ÿ
device = "mps" if torch.backends.mps.is_available() else "cpu"
sam.to(device=device)
```

### Q2: å¦‚ä½•æé«˜ç²¾åº¦ï¼Ÿ
```python
# 1. ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ (ViT-H)
# 2. æä¾›å¤šä¸ªç‚¹
input_points = np.array([[300, 200], [350, 250]])  # å¤šä¸ªç‚¹
input_labels = np.array([1, 1])  # éƒ½æ˜¯å‰æ™¯

# 3. ä½¿ç”¨è¾¹ç•Œæ¡†
input_box = np.array([x1, y1, x2, y2])  # æ¡†ä½ç›®æ ‡
```

### Q3: å¦‚ä½•åŒºåˆ†ä¸åŒçš„æœè£…éƒ¨ä½ï¼Ÿ
```python
# SAMæœ¬èº«ä¸è¯†åˆ«ç±»åˆ«ï¼Œåªåˆ†å‰²
# éœ€è¦ç»“åˆå…¶ä»–æ–¹æ³•ï¼š

# æ–¹æ³•1: è®©ç”¨æˆ·ç‚¹å‡»å¹¶æ ‡è®°
# æ–¹æ³•2: ç»“åˆCLIPè¿›è¡Œåˆ†ç±»
# æ–¹æ³•3: ä½¿ç”¨ Grounded-SAM (è‡ªåŠ¨è¯†åˆ«)
```

### Q4: èƒ½åœ¨æµè§ˆå™¨å®æ—¶è¿è¡Œå—ï¼Ÿ
```python
# å¯ä»¥ï¼æ­¥éª¤ï¼š
# 1. å¯¼å‡ºä¸ºONNX
# 2. ä½¿ç”¨ onnxruntime-web
# 3. éƒ¨ç½²åˆ°ç½‘é¡µ

# æˆ–è€…ç›´æ¥ç”¨ MobileSAM (æ›´é€‚åˆå®æ—¶)
```

---

## ğŸ”— ç›¸å…³èµ„æº

- **å®˜æ–¹è®ºæ–‡**: https://arxiv.org/abs/2304.02643
- **å®˜æ–¹åšå®¢**: https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/
- **åœ¨çº¿Demo**: https://segment-anything.com/demo
- **Hugging Face**: https://huggingface.co/facebook/sam-vit-base

---

## ğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®

### ç«‹å³ä½“éªŒ:
```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/facebookresearch/segment-anything.git
cd segment-anything

# 2. å®‰è£…
pip install -e .

# 3. ä¸‹è½½æ¨¡å‹
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth

# 4. è¿è¡Œç¤ºä¾‹
python scripts/amg.py --checkpoint sam_vit_b_01ec64.pth --input your_image.jpg --output output/
```

### é›†æˆåˆ°æ‚¨çš„é¡¹ç›®:
1. å…ˆè¿è¡Œä¸Šé¢çš„äº¤äº’ç¤ºä¾‹
2. æµ‹è¯•åœ¨æ‚¨çš„æœè£…å›¾ç‰‡ä¸Šçš„æ•ˆæœ
3. å¦‚æœæ»¡æ„ï¼Œæˆ‘å¸®æ‚¨é›†æˆåˆ°Webåº”ç”¨

---

## ğŸ¯ å¯¹æ¯”å½“å‰é¡¹ç›®

| ç‰¹æ€§ | å½“å‰é¡¹ç›®(SCHP) | SAM |
|------|---------------|-----|
| **äº¤äº’æ–¹å¼** | é¢„å…ˆåˆ†å‰²æ‰€æœ‰ | ç‚¹å‡»å³åˆ†å‰²â­ |
| **ç²¾åº¦** | 82% | 95%+ â­ |
| **è¾¹ç¼˜è´¨é‡** | ä¸€èˆ¬ | åƒç´ çº§å®Œç¾â­ |
| **çµæ´»æ€§** | å›ºå®š18ç±» | æ— é™åˆ¶â­ |
| **å“åº”é€Ÿåº¦** | éœ€è¦é‡æ–°è¿è¡Œ | å®æ—¶å“åº”â­ |
| **ç”¨æˆ·ä½“éªŒ** | ä¸€èˆ¬ | æä½³â­ |

**ç»“è®º**: SAM åœ¨äº¤äº’ä½“éªŒä¸Šå®Œå…¨ç¢¾å‹å½“å‰é¡¹ç›®ï¼

